{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "gzbwmdzuxpun73s636wj",
   "authorId": "8935411962970",
   "authorName": "DYJA",
   "authorEmail": "nur.khadijah@berca.co.id",
   "sessionId": "ea14e5f3-28f0-483b-9518-fa107be4fb37",
   "lastEditTime": 1761602399556
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "70031fab-33ad-4ebd-8368-d3d55aac9bf2",
   "metadata": {
    "language": "python",
    "name": "Snowpark_Session",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0c1e0afd-b6c4-4004-b78b-b50832d13406",
   "metadata": {
    "language": "python",
    "name": "Library"
   },
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport json, os\nfrom collections import Counter, defaultdict, deque\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (\n    classification_report, roc_auc_score, average_precision_score,\n    precision_recall_curve, roc_curve, brier_score_loss,\n    confusion_matrix, ConfusionMatrixDisplay, make_scorer\n)\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import GridSearchCV\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom snowflake.ml.registry import Registry",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8fa575a9-8416-4cd8-8eff-74145e2b222a",
   "metadata": {
    "language": "python",
    "name": "Option"
   },
   "outputs": [],
   "source": "RANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\npd.set_option('display.max_columns', None)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c6fd236-39ab-45b1-94b2-c103737ecfb3",
   "metadata": {
    "language": "python",
    "name": "Read_to_pandas"
   },
   "outputs": [],
   "source": "xf = session.table(\"HACKATHON.RAW.ML_FEATURES\").to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fa84038-a470-44ab-9f36-389252acd122",
   "metadata": {
    "language": "python",
    "name": "Re_run"
   },
   "outputs": [],
   "source": "df = xf.copy()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "857c132e-45b0-400c-a0ff-6663a9b74165",
   "metadata": {
    "language": "python",
    "name": "Convert_to_Lowercase"
   },
   "outputs": [],
   "source": "df.columns = df.columns.str.lower()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce04965a-bc5a-485c-8e36-2a4ac8285afe",
   "metadata": {
    "language": "python",
    "name": "Epoch_to_DateTime"
   },
   "outputs": [],
   "source": "df['timestamp_1h'] = pd.to_datetime(df['timestamp_1h'], unit='ns')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2cdbab2-28ab-4123-9771-cf9d1aefec02",
   "metadata": {
    "language": "python",
    "name": "Explore_Data"
   },
   "outputs": [],
   "source": "df.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dcaaed7d-218d-4289-851f-e2f1181dda81",
   "metadata": {
    "language": "python",
    "name": "Explore_Data_2"
   },
   "outputs": [],
   "source": "df.info()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad3af46a-961a-435f-a064-c47ce222e2b7",
   "metadata": {
    "language": "python",
    "name": "Parse_Time_n_Sort_Time"
   },
   "outputs": [],
   "source": "df = df.sort_values('timestamp_1h').reset_index(drop=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b36c2e1c-8bc1-4242-8988-ae4a7936b0c8",
   "metadata": {
    "language": "python",
    "name": "Sorting_Time"
   },
   "outputs": [],
   "source": "# Plot after sorting\nplt.figure(figsize=(10, 4))\nplt.plot(df['timestamp_1h'])\nplt.title('After Sorting')\nplt.xlabel('Index')\nplt.ylabel('timestamp_1h')\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cae65e57-7e94-414d-838d-003814e5cf94",
   "metadata": {
    "language": "python",
    "name": "Sort_by_Row_Number"
   },
   "outputs": [],
   "source": "# Sort by the column 'ROW_NUMBER' and reset the index\ndf = df.sort_values(by='row_number').reset_index(drop=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16489dc2-7ae4-4664-8c03-d3784bdb6a89",
   "metadata": {
    "language": "python",
    "name": "Explore",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df.head(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "236d0ae6-9ddf-48cf-9c0e-99ca6a8ea001",
   "metadata": {
    "language": "python",
    "name": "Explore_2"
   },
   "outputs": [],
   "source": "df.tail(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "efd99a73-568a-4d87-b45c-d93b1b733b0d",
   "metadata": {
    "language": "python",
    "name": "Splitting_dataset"
   },
   "outputs": [],
   "source": "# Dataset split:\n# - 20% for test\n# - 10% of the remaining data for validation\n# - 5% of the training data for calibration (make probability predictions more accurate)\ntest_frac = 0.20\nval_frac  = 0.10\ncal_frac  = 0.05\n\nn = len(df)\ntest_cut = int(n * (1 - test_frac))\npre_test = df.iloc[:test_cut].copy()\ntest_df  = df.iloc[test_cut:].copy()\n\nval_cut  = int(len(pre_test) * (1 - val_frac))\ntrain_df = pre_test.iloc[:val_cut].copy()\nval_df   = pre_test.iloc[val_cut:].copy()\n\ncal_cut  = int(len(train_df) * (1 - cal_frac))\ntrain_core = train_df.iloc[:cal_cut].copy()\ncal_df     = train_df.iloc[cal_cut:].copy()\n\nfor name, df_ in [(\"train_core\", train_core), (\"cal\", cal_df), (\"val\", val_df), (\"test\", test_df)]:\n    print(name, df_['timestamp_1h'].min(), \"→\", df_['timestamp_1h'].max(), \"rows:\", len(df_))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a084d161-e0fc-4c82-9b58-b5b48139735c",
   "metadata": {
    "language": "python",
    "name": "Percentage_Target_Variable"
   },
   "outputs": [],
   "source": "def show_target(df, name):\n    p = df['label_outage_1h'].mean()*100\n    print(f\"{name:<10} pos%={p:.4f}  (pos={df['label_outage_1h'].sum():,}, n={len(df):,})\")\n\nshow_target(train_core, \"train_core\")\nshow_target(cal_df,     \"cal\")\nshow_target(val_df,     \"val\")\nshow_target(test_df,    \"test\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab9b8e0d-dc20-48d9-b3b7-dbd31b284be5",
   "metadata": {
    "language": "python",
    "name": "Handling_outliers"
   },
   "outputs": [],
   "source": "# Numeric non-binary\nselected_feats = [\n    'ont_registered',\n    'offline_ont_now',\n    'offline_ont_ratio',\n    'link_loss_count',\n    'bad_rsl_count',\n    'high_temp_count',\n    'dying_gasp_count',\n    'trap_trend_score',\n    'fault_rate',\n    'snr_avg',\n    'rx_power_avg',\n    'rx_power_avg_dbm',\n    'temperature_avg_c',\n    'temp_anomaly_score',\n    'hour_of_day',\n    'day_of_week'\n]\n\n# Initialize all with None\ncapping_limits = {feature: None for feature in selected_feats}\n\ncapping_limits['ont_registered'] = [0, 100]\ncapping_limits['offline_ont_now'] = [0, 100] # extreme outliers, count\ncapping_limits['offline_ont_ratio'] = [0, 98] # extreme outliers\ncapping_limits['link_loss_count'] = [0, 100] # extreme outliers, count\ncapping_limits['bad_rsl_count'] = [0, 100] # extreme outliers, count\ncapping_limits['high_temp_count'] = [0, 100] # extreme outliers, count\ncapping_limits['dying_gasp_count'] = [0, 100] # extreme outliers, count\ncapping_limits['trap_trend_score'] = [1, 98] # extreme outliers\ncapping_limits['fault_rate'] = [0, 99.5] # extreme outliers\ncapping_limits['snr_avg'] = [0.5, 100] # extreme outliers\ncapping_limits['rx_power_avg'] = [0.15, 100] # extreme outliers\ncapping_limits['rx_power_avg_dbm'] = [0.15, 100] # extreme outliers\ncapping_limits['temperature_avg_c'] = [0.5, 99.5] # extreme outliers\ncapping_limits['temp_anomaly_score'] = [0.5, 99.5] # extreme outliers\ncapping_limits['hour_of_day'] = [0, 100]\ncapping_limits['day_of_week'] = [0, 100]\n\ndef cap_values(series, lower, upper):\n    return np.clip(series, lower, upper)\n\ncaps = {}\n\nfor col, (low_p, high_p) in capping_limits.items():\n    # Compute caps from TRAINING data only\n    lower_cap = -np.inf if low_p == 0 else np.percentile(train_core[col], low_p)\n    upper_cap = np.inf if high_p == 100 else np.percentile(train_core[col], high_p)\n    caps[col] = (lower_cap, upper_cap)\n\n    before_min, before_max = train_core[col].min(), train_core[col].max()\n    num_low = (train_core[col] < lower_cap).sum()\n    num_high = (train_core[col] > upper_cap).sum()\n\n    print(f\"\\n[{col}]\")\n    print(f\"  Before: min={before_min:.4f}, max={before_max:.4f}\")\n    print(f\"  Caps: lower={lower_cap:.4f}, upper={upper_cap:.4f}\")\n    print(f\"  Rows capped: low={num_low}, high={num_high}\")\n\n    for df in [train_core, cal_df, val_df, test_df]:\n        df[col] = cap_values(df[col], lower_cap, upper_cap)\n\n    after_min, after_max = train_core[col].min(), train_core[col].max()\n    print(f\"  After:  min={after_min:.4f}, max={after_max:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5fbaf63d-c3c9-43f6-830c-0a57f320dca2",
   "metadata": {
    "language": "python",
    "name": "Handling_Skewness"
   },
   "outputs": [],
   "source": "# The skewed count-like features\nskewed_feats = ['offline_ont_now', 'bad_rsl_count', 'link_loss_count', 'dying_gasp_count', 'high_temp_count']\n\n# Apply log1p transform safely (no drop)\nfor df in [train_core, cal_df, val_df, test_df]:\n    for col in skewed_feats:\n        # Add small epsilon to avoid all zeros\n        clipped = np.clip(df[col], 0, None)\n        df[f'{col}_log'] = np.log1p(clipped + 1e-10)  # Small epsilon\n\n# Build list of log-transformed feature names\nskewed_feats_log = [f'{col}_log' for col in skewed_feats]\n\n# Compute skew only for non-binary numeric columns (selected + log-transformed)\nskewed = train_core[selected_feats + skewed_feats_log].skew().sort_values(ascending=True)\n\n# Filter skewed Series to exclude features in skewed_feats\nskewed_filtered = skewed[~skewed.index.isin(skewed_feats)]\n\n# Print like a pandas Series\nfor feature, skew_value in skewed_filtered.items():\n    print(f\"{feature:<20} {skew_value:.3f}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7a0736f-0ecd-4e40-b1bd-30c72f0a5b79",
   "metadata": {
    "language": "python",
    "name": "Dropping_Multicollinearity_Cols"
   },
   "outputs": [],
   "source": "# Drop them manually from each dataframe\nfor df in [train_core, cal_df, val_df, test_df]:\n    df.drop(columns=skewed_feats, inplace=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e80f7604-d39d-4687-b43b-eb957e579988",
   "metadata": {
    "language": "python",
    "name": "Target_Distribution"
   },
   "outputs": [],
   "source": "def target_dist(df, col_target, target_1_label='Positive', target_0_label='Negative'):\n    # Smaller global font size\n    mpl.rcParams['font.size'] = 8\n\n    # Count values\n    r = df[col_target].value_counts().sort_index()\n    total = r.sum()\n\n    # Labels in correct order (0 → Negative, 1 → Positive)\n    labels = [target_0_label if i == 0 else target_1_label for i in r.index]\n    \n    # Custom function for % and count\n    def make_autopct(values):\n        def my_autopct(pct):\n            val = int(round(pct * total / 100.0))\n            return f\"{pct:.1f}%\\n({val:,})\"\n        return my_autopct\n\n    # Smaller figure and radius\n    fig, ax = plt.subplots(figsize=(2.2, 2.2))  # compact figure\n    wedges, texts, autotexts = ax.pie(\n        r,\n        explode=[0.02, 0.04],\n        labels=labels,\n        radius=0.9,\n        autopct=make_autopct(r),\n        shadow=False,\n        startangle=45,\n        colors=['#66b3ff', '#ff9999'],\n        textprops={'color': 'black', 'fontsize': 7}\n    )\n\n    # Clean styling\n    ax.set_aspect('equal')\n    ax.set_frame_on(False)\n    plt.setp(autotexts, size=6, weight=\"bold\", color=\"white\")\n\n    # Compact title and layout\n    plt.title(f\"{col_target} Distribution\", fontweight=\"bold\", fontsize=9, pad=6)\n    plt.tight_layout(pad=0.5)\n    plt.show()\n\n# Example usage\ntarget_dist(train_core, 'label_outage_1h', target_1_label='Outage', target_0_label='No Outage')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8df40e41-7b23-4b17-98e2-127f8855268a",
   "metadata": {
    "language": "python",
    "name": "Feature_Engineering"
   },
   "outputs": [],
   "source": "GROUP_KEY = 'olt_id'\nROLL_KEYS = [\n    'link_loss_count_log','bad_rsl_count_log','high_temp_count_log',\n    'dying_gasp_count_log','offline_ont_now_log'\n]\n\ndef _safe_numeric(df, cols):\n    f = df.copy()\n    for c in cols:\n        if c in f.columns:\n            f[c] = pd.to_numeric(f[c], errors='coerce')\n    # Replace inf/-inf from logs, then fill NaN later\n    f.replace([np.inf, -np.inf], np.nan, inplace=True)\n    return f\n\ndef add_time_feats(f):\n    f = f.copy()\n    f['hour_sin'] = np.sin(2*np.pi * f['hour_of_day']/24.0)\n    f['hour_cos'] = np.cos(2*np.pi * f['hour_of_day']/24.0)\n    return f\n\ndef add_roll_delta(f, group_key=GROUP_KEY, windows=(6, 24)):\n    f = _safe_numeric(f, ROLL_KEYS)\n    f = f.sort_values(['timestamp_1h', group_key]).copy()\n    # Deltas on LOG-space (interpretable as multiplicative change)\n    for col in ROLL_KEYS:\n        if col in f.columns:\n            f[col + \"_delta_1h\"] = f.groupby(group_key)[col].diff().astype(float)\n    # Rolling means on LOG-space (smooths multiplicative noise)\n    for w in windows:\n        for col in ROLL_KEYS:\n            if col in f.columns:\n                f[f\"{col}_roll{w}h_mean\"] = (\n                    f.groupby(group_key)[col]\n                     .rolling(w, min_periods=1).mean()\n                     .reset_index(level=0, drop=True)\n                     .astype(float)\n                )\n    return f\n\ndef prepare_block(df):\n    f = add_time_feats(df)\n    # ensure *_log present; if not, warn minimally\n    missing_logs = [c for c in ROLL_KEYS if c not in f.columns]\n    if missing_logs:\n        print(\"WARNING: missing log columns:\", missing_logs)\n    return f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9305b3b5-b290-4b7c-bbd1-80ba8f273682",
   "metadata": {
    "language": "python",
    "name": "Add_Features"
   },
   "outputs": [],
   "source": "# Prepare blocks\ntrain_core_f = prepare_block(train_core)\ncal_f        = prepare_block(cal_df)\nval_f        = prepare_block(val_df)\ntest_f       = prepare_block(test_df)\n\n# Compute roll/delta chronologically across all to avoid leakage\nconcat_all = pd.concat([train_core_f, cal_f, val_f, test_f], axis=0).sort_values('timestamp_1h')\nconcat_all = add_roll_delta(concat_all, group_key=GROUP_KEY, windows=(6,24))\n\ntrain_core_f = concat_all.loc[train_core_f.index].copy()\ncal_f        = concat_all.loc[cal_f.index].copy()\nval_f        = concat_all.loc[val_f.index].copy()\ntest_f       = concat_all.loc[test_f.index].copy()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c62ae93d-03e4-44c1-bc8e-6732176ee6df",
   "metadata": {
    "language": "python",
    "name": "Features"
   },
   "outputs": [],
   "source": "# Base numeric (no raw IDs)\nnum_base = [\n    'offline_ont_ratio','trap_trend_score','fault_rate',\n    'snr_avg','rx_power_avg_dbm','temperature_avg_c','temp_anomaly_score',\n    'hour_sin','hour_cos','is_maintenance_window'\n]\n\n# Build feature list: *_log deltas/rollings + base\nroll_cols = [c for c in train_core_f.columns if any(s in c for s in [\n    \"_delta_1h\",\"_roll6h_mean\",\"_roll24h_mean\"\n]) and any(c.startswith(k) for k in ROLL_KEYS)]\n\nfeature_cols_all = [c for c in num_base if c in train_core_f.columns] + roll_cols\n\n# Clean NaNs from log ops later during matrix build\ntarget_col = 'label_outage_1h'\nprint(\"Num features:\", len(feature_cols_all))\nprint(sorted(feature_cols_all)[:12], \"...\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5405fdb5-3cbe-4f2b-b711-b063afbdfbbb",
   "metadata": {
    "language": "python",
    "name": "Multicollinearity_Issue"
   },
   "outputs": [],
   "source": "col_to_drop = ['temp_anomaly_score']\n# Drop the column in-place for each DataFrame\nfor df in [train_core_f, cal_f, val_f, test_f]:\n    df.drop(columns=col_to_drop, inplace=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4e8d979-4ee7-4dee-b6b3-3694fd8a157d",
   "metadata": {
    "language": "python",
    "name": "Update_All_Cols_Features"
   },
   "outputs": [],
   "source": "feature_cols_all = [col for col in feature_cols_all if col not in col_to_drop]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d33e9dc1-9b1b-49f3-ac33-246f429c718d",
   "metadata": {
    "language": "python",
    "name": "Splitting_Vars"
   },
   "outputs": [],
   "source": "def make_xy(frame, feats, target):\n    X = frame[feats].copy()\n    # Replace NaNs introduced by logs/diffs/rolling\n    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(float)\n    y = frame[target].astype(int)\n    return X, y\n\nX_tr, y_tr = make_xy(train_core_f, feature_cols_all, target_col)\nX_cal, y_cal = make_xy(cal_f, feature_cols_all, target_col)\nX_va,  y_va  = make_xy(val_f,  feature_cols_all, target_col)\nX_te,  y_te  = make_xy(test_f, feature_cols_all, target_col)\n\nprint(\"Shapes:\", X_tr.shape, X_cal.shape, X_va.shape, X_te.shape)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0a63988-d1f3-49cb-845a-3c9d76b335aa",
   "metadata": {
    "language": "python",
    "name": "Convert_Number_Dtype"
   },
   "outputs": [],
   "source": "# Convert feature datasets\nX_tr = X_tr.astype('float32')\nX_cal = X_cal.astype('float32')\nX_va = X_va.astype('float32')\nX_te = X_te.astype('float32')\n\n# Convert label datasets\ny_tr = y_tr.astype('int8')\ny_cal = y_cal.astype('int8')\ny_va = y_va.astype('int8')\ny_te = y_te.astype('int8')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab49280f-3e3d-4269-98ea-7f2f4cbbd960",
   "metadata": {
    "language": "python",
    "name": "Tuning_Functions"
   },
   "outputs": [],
   "source": "def threshold_by_recall(y_true, y_proba, target_recall=0.30, min_precision=0.30):\n    \"\"\"\n    Choose threshold that reaches >= target_recall on validation.\n    Prefer the candidate with the highest precision; if none reach target,\n    pick the max-recall point (best effort).\n    \"\"\"\n    p, r, thr = precision_recall_curve(y_true, y_proba)\n    p, r = p[1:], r[1:]  # align with thresholds\n    idx = np.where(r >= target_recall)[0]\n    if len(idx) == 0:\n        best = int(np.argmax(r))\n        return float(thr[best]), float(p[best]), float(r[best])\n    ok = idx[p[idx] >= min_precision]\n    if len(ok) == 0:\n        ok = idx\n    best = ok[np.argmax(p[ok])]\n    return float(thr[best]), float(p[best]), float(r[best])\n\ndef scan_threshold_grid(y_true, y_proba, recall_targets, precision_floors):\n    \"\"\"\n    Grid-search over (target_recall, min_precision) pairs on validation.\n    Returns a DataFrame with the chosen threshold and the achieved (precision, recall)\n    for each pair, plus a composite score (F_beta) you can use to select the 'best'.\n    \"\"\"\n    rows = []\n    for tr in recall_targets:\n        for mp in precision_floors:\n            thr, p_va, r_va = threshold_by_recall(y_true, y_proba, tr, mp)\n            beta = 2.0  # emphasize recall (F2)\n            if (p_va + r_va) == 0:\n                fbeta = 0.0\n            else:\n                fbeta = (1+beta**2) * (p_va*r_va) / (beta**2*p_va + r_va)\n            rows.append({\n                \"target_recall\": tr,\n                \"min_precision\": mp,\n                \"threshold\": thr,\n                \"precision_val\": p_va,\n                \"recall_val\": r_va,\n                \"f2_val\": fbeta\n            })\n    return pd.DataFrame(rows).sort_values([\"f2_val\",\"recall_val\",\"precision_val\"], ascending=False)\n\ndef pick_best_by_recall(models_probas, y_true, recall_target=0.30, min_precision=0.30):\n    \"\"\"\n    models_probas: dict name -> (proba_va, proba_te)\n    Returns: dict of metrics keyed by model name using recall-first thresholding\n    \"\"\"\n    out = {}\n    for name, (proba_va, proba_te) in models_probas.items():\n        thr, p_va, r_va = threshold_by_recall(y_true, proba_va, recall_target, min_precision)\n        out[name] = (thr, p_va, r_va, proba_te)\n    return out",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "223df60a-b40f-4b75-a045-81e6bc6b0fc0",
   "metadata": {
    "language": "python",
    "name": "Parameter_Setup"
   },
   "outputs": [],
   "source": "models = {} # will fill with: name -> (calibrated_model, X_va_matrix, X_te_matrix)\nap_scorer = make_scorer(average_precision_score, needs_proba=True)\n\n# --- subsample 10% for tuning ---\nsample_frac = 0.10\nX_tune = X_tr.sample(frac=sample_frac, random_state=42)\ny_tune = y_tr.loc[X_tune.index]\nprint(f\"Tuning on {len(X_tune):,} rows ({sample_frac*100:.0f}%) of training data\")\n\npos_weight = (len(y_tr) - y_tr.sum()) / y_tr.sum()\nprint(f\"Class imbalance ratio ≈ {pos_weight:.1f}:1\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b85bd093-fd5a-4532-a6a7-d7e5fb729dfc",
   "metadata": {
    "language": "python",
    "name": "XGboost"
   },
   "outputs": [],
   "source": "xgb = XGBClassifier(\n    tree_method=\"hist\", n_jobs=-1, eval_metric=\"aucpr\",\n    random_state=RANDOM_STATE\n)\ngrid_xgb = {\n    \"n_estimators\": [400],\n    \"learning_rate\": [0.05],\n    \"max_depth\": [6, 8],\n    \"subsample\": [0.8],\n    \"colsample_bytree\": [0.8],\n    \"min_child_weight\": [1, 3],\n    \"scale_pos_weight\": [pos_weight]\n}\ngs_xgb = GridSearchCV(xgb, grid_xgb, scoring=ap_scorer, cv=2, n_jobs=1, verbose=1)\ngs_xgb.fit(X_tune, y_tune)\nbest_xgb = gs_xgb.best_estimator_\nbest_xgb.fit(X_tr, y_tr)\ncal_xgb = CalibratedClassifierCV(best_xgb, cv=\"prefit\", method=\"sigmoid\")\ncal_xgb.fit(X_cal, y_cal)\nmodels[\"XGBoost\"] = (cal_xgb, X_va, X_te)\nprint(\"XGBoost best:\", gs_xgb.best_params_)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc4c687d-70a7-4d04-abc9-f14eac570d0f",
   "metadata": {
    "language": "python",
    "name": "CatBoost"
   },
   "outputs": [],
   "source": "cat = CatBoostClassifier(loss_function=\"Logloss\", eval_metric=\"AUC\",\n                         random_seed=RANDOM_STATE, verbose=False, thread_count=1)\ngrid_cat = {\n    \"iterations\": [1200],\n    \"learning_rate\": [0.05],\n    \"depth\": [6, 8],\n    \"l2_leaf_reg\": [3.0],\n    \"class_weights\": [[1.0, pos_weight]]\n}\ngs_cat = GridSearchCV(cat, grid_cat, scoring=ap_scorer, cv=2, n_jobs=1, verbose=1)\ngs_cat.fit(X_tune, y_tune)\nbest_cat = gs_cat.best_estimator_\nbest_cat.fit(X_tr, y_tr)\ncal_cat = CalibratedClassifierCV(best_cat, cv=\"prefit\", method=\"sigmoid\")\ncal_cat.fit(X_cal, y_cal)\nmodels[\"CatBoost\"] = (cal_cat, X_va, X_te)\nprint(\"CatBoost best:\", gs_cat.best_params_)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb3d9036-3985-4171-bd48-de3d5858e9f0",
   "metadata": {
    "language": "python",
    "name": "Probability"
   },
   "outputs": [],
   "source": "# Cobmpute and cache probabilities for every model\nprobas = {}  # name -> dict with proba_va, proba_te, references to X sets\nfor name, (model, Xv, Xt) in models.items():\n    proba_va = model.predict_proba(Xv)[:, 1]\n    proba_te = model.predict_proba(Xt)[:, 1]\n    probas[name] = {\"proba_va\": proba_va, \"proba_te\": proba_te, \"Xv\": Xv, \"Xt\": Xt}",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5cf81559-6056-4302-bf66-6b50b427f263",
   "metadata": {
    "language": "python",
    "name": "Tuning_Recall"
   },
   "outputs": [],
   "source": "# Config\nrecall_grid = [0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90]\nprecision_grid = [0.20, 0.30, 0.40, 0.50, 0.60, 0.70]\nmax_diff = 0.05  # allowed difference between val and test metrics\n\nsummary_rows = []\nchosen_ops = {}\n\nfor name, d in probas.items():\n    df_scan = scan_threshold_grid(y_va, d[\"proba_va\"], recall_grid, precision_grid)\n    # Sort by F2 (best first), tie-break by recall, then precision\n    df_scan = df_scan.sort_values([\"f2_val\", \"recall_val\", \"precision_val\"], ascending=[False, False, False]).reset_index(drop=True)\n\n    found_valid = False\n\n    # Try each candidate threshold in order\n    for _, row in df_scan.iterrows():\n        thr = float(row[\"threshold\"])\n        y_pred_te = (d[\"proba_te\"] >= thr).astype(int)\n        tp = int(((y_te == 1) & (y_pred_te == 1)).sum())\n        fp = int(((y_te == 0) & (y_pred_te == 1)).sum())\n        prec_test = tp / max(tp + fp, 1)\n        rec_test  = tp / max(int(y_te.sum()), 1)\n\n        diff_prec = abs(float(row[\"precision_val\"]) - prec_test)\n        diff_rec  = abs(float(row[\"recall_val\"]) - rec_test)\n        overfit_flag = (diff_prec > max_diff) or (diff_rec > max_diff)\n\n        if not overfit_flag:\n            # ✅ Found a non-overfitting threshold\n            chosen_ops[name] = {\n                \"thr\": thr,\n                \"precision_val\": float(row[\"precision_val\"]),\n                \"recall_val\": float(row[\"recall_val\"]),\n                \"target_recall\": float(row[\"target_recall\"]),\n                \"min_precision\": float(row[\"min_precision\"]),\n                \"f2_val\": float(row[\"f2_val\"])\n            }\n            thr_for_summary = thr\n            note = f\"✅ Selected (no overfit, diff≤{max_diff})\"\n            found_valid = True\n            break\n\n    if not found_valid:\n        # ❌ No valid threshold found for this model\n        thr_for_summary = None\n        note = f\"⚠️ All thresholds overfit (val/test diff > {max_diff})\"\n        chosen_ops[name] = {\n            \"thr\": None,\n            \"precision_val\": None,\n            \"recall_val\": None,\n            \"target_recall\": None,\n            \"min_precision\": None,\n            \"f2_val\": None\n        }\n\n    # For summary reporting\n    if found_valid:\n        best_row = row\n        prec_test_final = prec_test\n        rec_test_final = rec_test\n        diff_prec_final = diff_prec\n        diff_rec_final = diff_rec\n        overfit_flag_final = False\n    else:\n        best_row = df_scan.iloc[0]  # fallback for reporting\n        y_pred_te = (d[\"proba_te\"] >= float(best_row[\"threshold\"])).astype(int)\n        tp = int(((y_te == 1) & (y_pred_te == 1)).sum())\n        fp = int(((y_te == 0) & (y_pred_te == 1)).sum())\n        prec_test_final = tp / max(tp + fp, 1)\n        rec_test_final  = tp / max(int(y_te.sum()), 1)\n        diff_prec_final = abs(float(best_row[\"precision_val\"]) - prec_test_final)\n        diff_rec_final = abs(float(best_row[\"recall_val\"]) - rec_test_final)\n        overfit_flag_final = True\n\n    summary_rows.append({\n        \"model\": name,\n        \"thr\": thr_for_summary,\n        \"policy_val\": f\"rec≥{float(best_row['target_recall']):.2f} & prec≥{float(best_row['min_precision']):.2f}\",\n        \"precision_val@thr\": float(best_row[\"precision_val\"]),\n        \"recall_val@thr\": float(best_row[\"recall_val\"]),\n        \"f2_val\": float(best_row[\"f2_val\"]),\n        \"pr_auc_test\": average_precision_score(y_te, d[\"proba_te\"]),\n        \"roc_auc_test\": roc_auc_score(y_te, d[\"proba_te\"]),\n        \"precision_test@thr\": prec_test_final,\n        \"recall_test@thr\": rec_test_final,\n        \"alerts_test\": int((d[\"proba_te\"] >= (thr_for_summary or 0)).sum()),\n        \"positives_test\": int(y_te.sum()),\n        \"overfitting\": overfit_flag_final,\n        \"diff_prec\": diff_prec_final,\n        \"diff_rec\": diff_rec_final,\n        \"note\": note\n    })",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e628e8ba-4a97-439a-a553-0a8147c16caf",
   "metadata": {
    "language": "python",
    "name": "Results"
   },
   "outputs": [],
   "source": "# === Build and filter summary ===\nresults_df = pd.DataFrame(summary_rows).sort_values(\n    [\"pr_auc_test\", \"recall_test@thr\", \"precision_test@thr\"],\n    ascending=[False, False, False]\n).reset_index(drop=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0c6988bf-66d7-4c4e-8cb5-ad746eddd0a2",
   "metadata": {
    "language": "python",
    "name": "Results_2"
   },
   "outputs": [],
   "source": "results_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee2f2e40-8117-4f9e-b189-5c5975dc1ff5",
   "metadata": {
    "language": "python",
    "name": "Valid_Results"
   },
   "outputs": [],
   "source": "# Keep only non-overfitting (valid) models\nvalid_results_df = results_df[results_df[\"overfitting\"] == False].copy().sort_values(\n    [\"pr_auc_test\", \"recall_test@thr\", \"precision_test@thr\"],\n    ascending=[False, False, False]\n).reset_index(drop=True)\n\nif valid_results_df.empty:\n    print(\"No valid models found — all thresholds overfit.\")\n    best_model = None\nelse:\n    best_model = valid_results_df.iloc[0].to_dict()\n    print(f\"✅ Best valid model: {best_model['model']} (thr={best_model['thr']})\")\n\nbest_name = valid_results_df.iloc[0][\"model\"]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb3ff3cd-9166-49c3-a383-fbad147a182f",
   "metadata": {
    "language": "python",
    "name": "Best_Model_Metrics"
   },
   "outputs": [],
   "source": "valid_results_df[['model', 'thr', 'recall_test@thr', 'pr_auc_test', 'precision_test@thr']]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5513c98-9915-476a-9fba-877aff0cfd98",
   "metadata": {
    "language": "python",
    "name": "Confusion_Matrix"
   },
   "outputs": [],
   "source": "best_thr = chosen_ops[best_name][\"thr\"]\nbest_model, Xv_best, Xt_best = models[best_name]\n\ny_proba_best_te = probas[best_name][\"proba_te\"]\ny_pred_best = (y_proba_best_te >= best_thr).astype(int)\n\n# Compute confusion matrix\ncm = confusion_matrix(y_te, y_pred_best)\ntn, fp, fn, tp = cm.ravel()\nlabels = np.array([[\"TN\", \"FP\"], [\"FN\", \"TP\"]])\n\n# Compact confusion matrix plot\nfig, ax = plt.subplots(figsize=(3.2, 3))  # reduced from (5.5, 5)\nim = ax.imshow(cm, interpolation='nearest', cmap=\"Blues\")\n\n# Label axes\nax.set_xticks([0, 1])\nax.set_yticks([0, 1])\nax.set_xticklabels([\"No Outage\", \"Outage\"], fontsize=6)\nax.set_yticklabels([\"No Outage\", \"Outage\"], fontsize=6)\nax.set_xlabel(\"Predicted\", fontsize=6)\nax.set_ylabel(\"True\", fontsize=6)\n\n# Compact title\nax.set_title(\n    f\"Confusion Matrix — {best_name}\\n\"\n    f\"thr={best_thr:.3f} | rec≥{chosen_ops[best_name]['target_recall']:.2f}, \"\n    f\"prec≥{chosen_ops[best_name]['min_precision']:.2f}\",\n    fontsize=7, pad=6\n)\n\n# Annotate cells with smaller text and color contrast\nfor (i, j), value in np.ndenumerate(cm):\n    color = \"white\" if value > cm.max() / 2 else \"black\"\n    ax.text(j, i, f\"{labels[i,j]}\\n{value:,}\",\n            ha='center', va='center', fontsize=6, fontweight='bold', color=color)\n\n# Smaller colorbar for compact layout\ncbar = plt.colorbar(im, ax=ax, fraction=0.04, pad=0.02)\ncbar.ax.tick_params(labelsize=5)\n\nplt.tight_layout(pad=0.3)\nplt.show()\n\n# Compute and print summary metrics\nprec = tp / max(tp + fp, 1)\nrec  = tp / max(tp + fn, 1)\nf1   = (2 * prec * rec) / max(prec + rec, 1e-12)\n\nprint(f\"TN={tn:,}  FP={fp:,}  FN={fn:,}  TP={tp:,}\")\nprint(f\"Precision={prec:.4f}  Recall={rec:.4f}  F1={f1:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "272499b1-5250-4f66-99fe-c4d596d42998",
   "metadata": {
    "language": "python",
    "name": "PR_Curve"
   },
   "outputs": [],
   "source": "proba_te_best = best_model.predict_proba(Xt_best)[:, 1]\n\n# Compute precision–recall curve\np, r, thr = precision_recall_curve(y_te, proba_te_best)\nap = average_precision_score(y_te, proba_te_best)\n\n# Find the precision and recall corresponding to the chosen threshold\n# (Note: thresholds returned by precision_recall_curve are for proba >= thr)\nidx = np.argmin(np.abs(thr - best_thr))  # closest threshold\nbest_p, best_r = p[idx], r[idx]\n\n# Compact Precision–Recall Curve\nplt.figure(figsize=(3.5, 3))  # reduced from (7, 6)\nplt.plot(r, p, label=f\"{best_name} (AP={ap:.3f})\", lw=1.2)\nplt.scatter(best_r, best_p, color=\"red\", s=30, zorder=5, label=f\"Best thr={best_thr:.2f}\")\n\n# Labels and title (smaller fonts)\nplt.xlabel(\"Recall\", fontsize=7)\nplt.ylabel(\"Precision\", fontsize=7)\nplt.title(\"Precision–Recall Curve (Test)\", fontsize=8, pad=4)\n\n# Smaller tick labels\nplt.tick_params(axis='both', which='major', labelsize=6)\nplt.tick_params(axis='both', which='minor', labelsize=5)\n\n# Legend and grid\nplt.legend(fontsize=6, loc=\"upper right\", frameon=False)\nplt.grid(True, linestyle='--', alpha=0.5, linewidth=0.6)\n\nplt.tight_layout(pad=0.3)\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b7dd2d8-e35e-4f5f-8c63-2d9299d6a6ff",
   "metadata": {
    "language": "python",
    "name": "Feature_Importance"
   },
   "outputs": [],
   "source": "if best_name == \"Logistic\":\n    # Xt_best is a scaled array; wrap with feature_cols_all names\n    Xt_best_df = pd.DataFrame(Xt_best, columns=feature_cols_all)\nelse:\n    # Xt_best is already a DataFrame with full feature columns\n    Xt_best_df = Xt_best.copy()\n\nX_pi = Xt_best_df.copy()\ny_pi = y_te.copy()\n\nperm = permutation_importance(\n    best_model, X_pi, y_pi,\n    n_repeats=3, random_state=RANDOM_STATE, n_jobs=1,\n    scoring=\"average_precision\"\n)\n\nimp = pd.DataFrame({\n    \"feature\": X_pi.columns,\n    \"importance_mean\": perm.importances_mean,\n    \"importance_std\": perm.importances_std\n}).sort_values(\"importance_mean\", ascending=False)\n\nprint(f\"Best model: {best_name}\")\nprint(imp.head(15))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0d5891ec-950f-4036-807d-c18a0f475cf3",
   "metadata": {
    "language": "python",
    "name": "Feature_Importance_Plot"
   },
   "outputs": [],
   "source": "# Select top 10 features\nimp_top10 = imp.head(10)\n\n# Horizontal bar plot\nplt.figure(figsize=(5, 3))\nplt.barh(imp_top10[\"feature\"], imp_top10[\"importance_mean\"], xerr=imp_top10[\"importance_std\"], color='skyblue')\nplt.xlabel(\"Permutation Importance (Mean ± Std)\")\nplt.ylabel(\"Feature\")\nplt.title(f\"Top 10 Permutation Feature Importances - {best_name} Model\")\nplt.gca().invert_yaxis()  # largest importance on top\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ceeb11e-f7b9-4554-937b-6681b6c6e0da",
   "metadata": {
    "language": "python",
    "name": "Save_Model"
   },
   "outputs": [],
   "source": "registry = Registry(session=session)\n\n# pick a small sample to avoid uploading entire dataset\nsample_input = pd.DataFrame(Xv_best).head(50)\n\nbest_name = best_name.lower()\n\n# Ensure valid dependency mapping\ndep_name = \"catboost\" if \"catboost\" in best_name else best_name\n\nmodel_version = registry.log_model(\n    model=best_model,\n    model_name=f\"{best_name}_outage_predictor\",\n    version_name=\"v1\",\n    conda_dependencies=[\"scikit-learn\", dep_name],\n    sample_input_data=sample_input\n)\n\nprint(\"✅ Model registered!\")\nprint(f\"Model Name: {model_version.model_name}\")\nprint(f\"Version: {model_version.version_name}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "431ba033-d21d-4306-8d0d-b1691c884101",
   "metadata": {
    "language": "python",
    "name": "Load_n_Inference"
   },
   "outputs": [],
   "source": "# Get the model first\nmodel = registry.get_model(f\"{best_name}_outage_predictor\")\n\n# Get the specific version from the model object\nmodel_version = model.version(\"v1\")\n\n# Load the actual Python model artifact\nloaded_model = model_version.load()\n\n# Run inference\ny_pred_new = loaded_model.predict(Xt_best)\ny_pred_new[:5]",
   "execution_count": null
  }
 ]
}