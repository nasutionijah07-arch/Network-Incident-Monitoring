{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38cb53c9-1e39-4e3c-8d10-66a9456d74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, hashlib\n",
    "\n",
    "# ------------------- Config --------------------\n",
    "np.random.seed(7)\n",
    "\n",
    "start = pd.Timestamp(\"2023-01-01 00:00:00\")\n",
    "end   = pd.Timestamp(\"2024-12-31 23:00:00\")\n",
    "freq = \"1h\"\n",
    "\n",
    "# Target prevalence control (approximate, global)\n",
    "TARGET_OUTAGE_RATE = 0.003  # ~0.3% hours positive; adjust 0.002â€“0.006 for realism\n",
    "\n",
    "# Primary geography now = PROVINCES (kept under column name \"city\" for schema stability)\n",
    "provinces = [\n",
    "    \"Aceh\",\"Bali\",\"Bangka Belitung\",\"Banten\",\"Bengkulu\",\"Gorontalo\",\"Jakarta Raya\",\"Jambi\",\n",
    "    \"Jawa Barat\",\"Jawa Tengah\",\"Jawa Timur\",\"Kalimantan Barat\",\"Kalimantan Selatan\",\"Kalimantan Tengah\",\n",
    "    \"Kalimantan Timur\",\"Kalimantan Utara\",\"Kepulauan Riau\",\"Lampung\",\"Maluku\",\"Maluku Utara\",\n",
    "    \"Nusa Tenggara Barat\",\"Nusa Tenggara Timur\",\"Papua\",\"Papua Barat\",\"Riau\",\n",
    "    \"Sulawesi Barat\",\"Sulawesi Selatan\",\"Sulawesi Tengah\",\"Sulawesi Tenggara\",\"Sulawesi Utara\",\n",
    "    \"Sumatera Barat\",\"Sumatera Selatan\",\"Sumatera Utara\",\"Yogyakarta\"\n",
    "]\n",
    "\n",
    "province_map = {p: p for p in provinces}\n",
    "\n",
    "# Macro-region by province\n",
    "region_map = {\n",
    "    # Sumatra\n",
    "    \"Aceh\": \"Sumatra\",\"Bangka Belitung\": \"Sumatra\",\"Bengkulu\": \"Sumatra\",\"Jambi\": \"Sumatra\",\n",
    "    \"Kepulauan Riau\": \"Sumatra\",\"Lampung\": \"Sumatra\",\"Riau\": \"Sumatra\",\n",
    "    \"Sumatera Barat\": \"Sumatra\",\"Sumatera Selatan\": \"Sumatra\",\"Sumatera Utara\": \"Sumatra\",\n",
    "    # Java\n",
    "    \"Banten\": \"Java\",\"Jakarta Raya\": \"Java\",\"Jawa Barat\": \"Java\",\"Jawa Tengah\": \"Java\",\n",
    "    \"Jawa Timur\": \"Java\",\"Yogyakarta\": \"Java\",\n",
    "    # Kalimantan\n",
    "    \"Kalimantan Barat\": \"Kalimantan\",\"Kalimantan Selatan\": \"Kalimantan\",\"Kalimantan Tengah\": \"Kalimantan\",\n",
    "    \"Kalimantan Timur\": \"Kalimantan\",\"Kalimantan Utara\": \"Kalimantan\",\n",
    "    # Sulawesi\n",
    "    \"Gorontalo\": \"Sulawesi\",\"Sulawesi Barat\": \"Sulawesi\",\"Sulawesi Selatan\": \"Sulawesi\",\n",
    "    \"Sulawesi Tengah\": \"Sulawesi\",\"Sulawesi Tenggara\": \"Sulawesi\",\"Sulawesi Utara\": \"Sulawesi\",\n",
    "    # Bali & Nusa Tenggara\n",
    "    \"Bali\": \"Bali & Nusa Tenggara\",\"Nusa Tenggara Barat\": \"Bali & Nusa Tenggara\",\"Nusa Tenggara Timur\": \"Bali & Nusa Tenggara\",\n",
    "    # Maluku\n",
    "    \"Maluku\": \"Maluku\",\"Maluku Utara\": \"Maluku\",\n",
    "    # Papua\n",
    "    \"Papua\": \"Papua\",\"Papua Barat\": \"Papua\",\n",
    "}\n",
    "\n",
    "# Collision-proof short code per province for IDs\n",
    "def province_code(name: str) -> str:\n",
    "    tokens = re.findall(r\"[A-Za-z]+\", name)\n",
    "    acro = \"\".join(t[0] for t in tokens).upper()\n",
    "    if len(acro) < 3:\n",
    "        raw = re.sub(r\"[^A-Za-z]\", \"\", name).upper()\n",
    "        acro = (raw + \"XXX\")[:3]\n",
    "    else:\n",
    "        acro = (acro + \"XXX\")[:3]\n",
    "    h2 = hashlib.md5(name.encode(\"utf-8\")).hexdigest()[:2].upper()\n",
    "    return f\"{acro}{h2}\"\n",
    "\n",
    "olts_per_city = 2\n",
    "clusters_per_olt = 3\n",
    "\n",
    "time_index = pd.date_range(start, end, freq=freq)\n",
    "T = len(time_index)\n",
    "\n",
    "hours = time_index.hour.values\n",
    "dows = time_index.dayofweek.values\n",
    "months = time_index.month.values\n",
    "doys = time_index.dayofyear.values\n",
    "\n",
    "# ----------------- Seasonality -----------------\n",
    "def diurnal_curve(hour_arr):\n",
    "    # Busy around 20:00, smaller midday bump\n",
    "    return 0.6 + 0.5*np.exp(-((hour_arr-20)%24)**2/(2*3**2)) + 0.2*np.exp(-((hour_arr-12)%24)**2/(2*4**2))\n",
    "\n",
    "def weekly_curve(dow_arr):\n",
    "    # Weekend bias\n",
    "    return 1.0 + 0.05*np.isin(dow_arr, [5,6]).astype(float)\n",
    "\n",
    "def seasonal_factor(month_arr, doy_arr):\n",
    "    # Yearly + rainy season\n",
    "    year_wave = 1.0 + 0.1*np.sin(2*np.pi*doy_arr/365.25 - 0.7)\n",
    "    rainy = 1.0 + 0.2*np.isin(month_arr, [11,12,1,2,3]).astype(float)\n",
    "    return year_wave * rainy\n",
    "\n",
    "diurnal = diurnal_curve(hours)\n",
    "weekly = weekly_curve(dows)\n",
    "season = seasonal_factor(months, doys)\n",
    "\n",
    "# Maintenance flags (slightly higher night-time probability)\n",
    "p_maint = 0.002*(1 + ((hours>=1)&(hours<=4)).astype(int))\n",
    "is_maint_arr = (np.random.rand(T) < p_maint).astype(int)\n",
    "\n",
    "# Province pressure spikes (macro load bursts)\n",
    "city_pressure = {}\n",
    "for prov in provinces:\n",
    "    cp = np.full(T, 0.02, dtype=float)\n",
    "    k = 40\n",
    "    idx = np.random.choice(T, size=k, replace=False)\n",
    "    for i in idx:\n",
    "        width = np.random.randint(3, 24)\n",
    "        amp = np.random.uniform(0.3, 0.8)\n",
    "        s = max(0, i-width//2); e = min(T, i+width//2)\n",
    "        cp[s:e] += amp * np.hanning(e-s)\n",
    "    city_pressure[prov] = cp\n",
    "\n",
    "# ----------------------------\n",
    "# Dimension keys\n",
    "# ----------------------------\n",
    "city_list = sorted(set(provinces))  # \"city\" holds province names\n",
    "city_key_map = {c:i+1 for i,c in enumerate(city_list)}\n",
    "\n",
    "# Entities with topology names (derive OLT/cluster keys)\n",
    "entities = []\n",
    "for prov in provinces:\n",
    "    code = province_code(prov)\n",
    "    for o in range(1, olts_per_city+1):\n",
    "        olt_id = f\"OLT-{code}-{o:02d}\"\n",
    "        fdt_name = olt_id.replace(\"OLT\",\"FDT\") + \"-A\"\n",
    "        fat_name = olt_id.replace(\"OLT\",\"FAT\") + \"-B\"\n",
    "        for c in range(1, clusters_per_olt+1):\n",
    "            cluster_id = f\"{code}-CL{o}{c}\"\n",
    "            cluster_name = f\"Cluster-{code}-{o}{c}\"\n",
    "            base_onts = np.random.randint(350, 900)\n",
    "            entities.append(dict(\n",
    "                city=prov,\n",
    "                city_key=city_key_map[prov],\n",
    "                province=province_map[prov],\n",
    "                region=region_map[prov],\n",
    "                olt_id=olt_id, fdt_name=fdt_name,\n",
    "                fat_name=fat_name, cluster_id=cluster_id, cluster_name=cluster_name,\n",
    "                base_onts=base_onts\n",
    "            ))\n",
    "entities_df = pd.DataFrame(entities)\n",
    "\n",
    "# Create OLT and Cluster keys\n",
    "olt_key_map = {oid:i+1 for i, oid in enumerate(sorted(entities_df['olt_id'].unique()))}\n",
    "cluster_key_map = {cid:i+1 for i, cid in enumerate(sorted(entities_df['cluster_id'].unique()))}\n",
    "entities_df['olt_key'] = entities_df['olt_id'].map(olt_key_map)\n",
    "entities_df['cluster_key'] = entities_df['cluster_id'].map(cluster_key_map)\n",
    "\n",
    "# =========================\n",
    "# Helper: latent stress AR\n",
    "# =========================\n",
    "def latent_stress_series(T, rng, base_level=0.05, ar=0.95, shock_prob=0.002, shock_scale=0.5):\n",
    "    \"\"\"\n",
    "    Generates a bounded [0,1] latent stress with AR(1) dynamics and occasional shocks.\n",
    "    \"\"\"\n",
    "    s = np.zeros(T, dtype=float)\n",
    "    s[0] = base_level\n",
    "    for t in range(1, T):\n",
    "        shock = (rng.random() < shock_prob) * rng.uniform(0.2, shock_scale)\n",
    "        noise = rng.normal(0, 0.02)\n",
    "        s[t] = ar*s[t-1] + (1-ar)*base_level + shock + noise\n",
    "        s[t] = np.clip(s[t], 0, 1.5)\n",
    "    return s.clip(0,1)\n",
    "\n",
    "# ======================================\n",
    "# Row generation with precursors\n",
    "# ======================================\n",
    "rows = []\n",
    "\n",
    "for _, ent in entities_df.iterrows():\n",
    "    prov = ent['city']\n",
    "    olt_id = ent['olt_id']; cluster_id = ent['cluster_id']\n",
    "    fdt_name = ent['fdt_name']; fat_name = ent['fat_name']; cluster_name = ent['cluster_name']\n",
    "    base_onts = ent['base_onts']\n",
    "    growth = np.linspace(0, np.random.randint(20,120), T).astype(int)\n",
    "    ont_registered = base_onts + growth\n",
    "\n",
    "    rng = np.random.default_rng(hash((prov,olt_id,cluster_id)) & 0xFFFFFFFF)\n",
    "\n",
    "    # Latent stress combines AR process + macro factors\n",
    "    stress = latent_stress_series(\n",
    "        T, rng,\n",
    "        base_level=0.05 + 0.05*np.mean(season) + 0.03*np.mean(weekly),\n",
    "        ar=rng.uniform(0.94, 0.98),\n",
    "        shock_prob=0.0025,\n",
    "        shock_scale=0.6\n",
    "    )\n",
    "    # Add diurnal/seasonal/province spikes\n",
    "    stress = np.clip(\n",
    "        (0.6*stress +\n",
    "         0.15*(diurnal-0.6) +\n",
    "         0.10*(season-1.0) +\n",
    "         0.15*city_pressure[prov]), 0, 1.2\n",
    "    )\n",
    "\n",
    "    # Base hazard tuned towards target; boosted by maintenance and stress\n",
    "    base_hazard = TARGET_OUTAGE_RATE * rng.uniform(0.8, 1.25)\n",
    "    hazard = (\n",
    "        base_hazard\n",
    "        * (1 + 2.5*stress)\n",
    "        * (1 + 0.9*is_maint_arr)\n",
    "        * (1 + 0.15*(hours == 3))\n",
    "    )\n",
    "    hazard = np.clip(hazard, TARGET_OUTAGE_RATE*0.2, 0.08)\n",
    "\n",
    "    # Sample outages\n",
    "    outage = (rng.random(T) < hazard).astype(int)\n",
    "\n",
    "    # Precursor window: when outage at t, we increase counters at t-1..t-3\n",
    "    precursor_span = rng.integers(1, 4)  # 1â€“3 hours\n",
    "    precursor = np.zeros(T, dtype=int)\n",
    "    outage_idx = np.where(outage==1)[0]\n",
    "    for t0 in outage_idx:\n",
    "        s = max(0, t0 - precursor_span)\n",
    "        precursor[s:t0] = 1\n",
    "\n",
    "    # Baseline event rates (per ONT per hour)\n",
    "    base_link_loss = rng.uniform(0.0004, 0.0012)\n",
    "    base_bad_rsl   = rng.uniform(0.0005, 0.0016)\n",
    "    base_temp      = rng.uniform(0.0002, 0.0010)\n",
    "    base_dying     = rng.uniform(0.0001, 0.0009)\n",
    "\n",
    "    maint_mult = 1.0 + 0.6*is_maint_arr\n",
    "    # Strengthen pre-outage signal by adding precursor multipliers\n",
    "    link_loss_rate = (base_link_loss*diurnal*weekly*season*maint_mult\n",
    "                      * (1 + 1.0*precursor + 2.5*outage))\n",
    "    bad_rsl_rate   = (base_bad_rsl*(0.6 + 0.6*season)\n",
    "                      * (1 + 0.8*precursor + 2.0*outage))\n",
    "    temp_rate      = (base_temp*(0.7 + 0.4*diurnal)\n",
    "                      * (1 + 0.5*precursor + 1.5*outage))\n",
    "    dying_rate     = (base_dying*(0.8 + 0.5*weekly)\n",
    "                      * (1 + 0.7*precursor + 2.2*outage))\n",
    "\n",
    "    # Counts\n",
    "    link_loss_cnt = rng.poisson(link_loss_rate*ont_registered)\n",
    "    bad_rsl_cnt   = rng.poisson(bad_rsl_rate*ont_registered)\n",
    "    high_temp_cnt = rng.poisson(temp_rate*ont_registered)\n",
    "    dying_cnt     = rng.poisson(dying_rate*ont_registered)\n",
    "\n",
    "    # Offline ratio affected by precursor/outage and congestion\n",
    "    offline_ratio = np.clip(\n",
    "        0.04*precursor + 0.22*outage\n",
    "        + 0.35*(link_loss_cnt+bad_rsl_cnt)/np.maximum(1, ont_registered)\n",
    "        + rng.normal(0.003,0.004,T),\n",
    "        0, 0.35\n",
    "    )\n",
    "    offline_ont = np.round(offline_ratio * ont_registered).astype(int)\n",
    "\n",
    "    # Trap/trend reacts to first differences (precursor emphasized)\n",
    "    trap_rate = (link_loss_cnt+bad_rsl_cnt+high_temp_cnt+dying_cnt)/np.maximum(1, ont_registered)\n",
    "    prev = np.r_[trap_rate[0], trap_rate[:-1]]\n",
    "    trap_trend = np.clip(6*(trap_rate - prev) + 0.8*precursor + 1.2*outage, -1.0, 2.0)\n",
    "    alarm_spike_flag = ((trap_rate > (0.008 + 0.7*hazard)) | (precursor==1) | (outage==1)).astype(int)\n",
    "\n",
    "    # Physical indicators with pre-outage drift\n",
    "    snr_avg = (26\n",
    "               + 2*np.sin(2*np.pi*(doys/365.25))\n",
    "               - 1.8*precursor - 5.5*outage\n",
    "               + rng.normal(0,0.7,T))\n",
    "    rx_power = (-19.0\n",
    "                + 1.0*np.sin(2*np.pi*((doys+30)/365.25))\n",
    "                - 0.8*precursor - 2.8*outage\n",
    "                + rng.normal(0,0.5,T))\n",
    "    temperature = (36\n",
    "                   + 2.2*diurnal\n",
    "                   + 0.5*np.sin(2*np.pi*(doys/365.25))\n",
    "                   + 0.6*precursor + 5.5*outage\n",
    "                   + rng.normal(0,0.8,T))\n",
    "    temp_anom = (temperature - 38)/2.5\n",
    "\n",
    "    df_ent = pd.DataFrame({\n",
    "        \"timestamp_1h\": time_index,\n",
    "        \"city\": prov,\n",
    "        \"city_key\": ent['city_key'],\n",
    "        \"province\": province_map[prov],\n",
    "        \"region\": region_map[prov],\n",
    "        \"olt_id\": olt_id, \"olt_key\": ent['olt_key'],\n",
    "        \"fdt_name\": fdt_name, \"fat_name\": fat_name,\n",
    "        \"cluster_id\": cluster_id, \"cluster_key\": ent['cluster_key'], \"cluster_name\": cluster_name,\n",
    "        \"ont_registered\": ont_registered.astype(int),\n",
    "        \"offline_ont_now\": offline_ont.astype(int),\n",
    "        \"offline_ont_ratio\": offline_ratio.astype(float),\n",
    "        \"link_loss_count\": link_loss_cnt.astype(int),\n",
    "        \"bad_rsl_count\": bad_rsl_cnt.astype(int),\n",
    "        \"high_temp_count\": high_temp_cnt.astype(int),\n",
    "        \"dying_gasp_count\": dying_cnt.astype(int),\n",
    "        \"alarm_spike_flag\": alarm_spike_flag.astype(int),\n",
    "        \"trap_trend_score\": trap_trend.astype(float),\n",
    "        \"snr_avg\": snr_avg.astype(float),\n",
    "        \"rx_power_avg\": rx_power.astype(float),  # dBm\n",
    "        \"rx_power_avg_dbm\": rx_power.astype(float),\n",
    "        \"temperature_avg_c\": temperature.astype(float),  # Celsius\n",
    "        \"temp_anomaly_score\": temp_anom.astype(float),\n",
    "        \"hour_of_day\": hours,\n",
    "        \"day_of_week\": dows,\n",
    "        \"is_maintenance_window\": is_maint_arr,\n",
    "        \"outage_now\": outage.astype(int),\n",
    "        \"precursor_flag\": precursor.astype(int),\n",
    "        \"stress\": stress.astype(float)\n",
    "    })\n",
    "    rows.append(df_ent)\n",
    "\n",
    "realtime = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# Hourly fault_rate from raw counts\n",
    "alarm_sum = realtime[['link_loss_count','bad_rsl_count','high_temp_count','dying_gasp_count']].sum(axis=1).astype(float)\n",
    "realtime['fault_rate'] = (alarm_sum / np.maximum(1, realtime['ont_registered'])).clip(lower=0)\n",
    "\n",
    "# ---------------- Context table ----------------\n",
    "context = realtime[['city','city_key','province','region','olt_id','olt_key','fdt_name','fat_name','cluster_id','cluster_key','cluster_name']]\\\n",
    "    .drop_duplicates().reset_index(drop=True)\n",
    "vendors = ['Huawei','ZTE','Nokia']\n",
    "context['vendor'] = [vendors[i % len(vendors)] for i in range(len(context))]\n",
    "context['firmware_version'] = 'v5.' + (10 + pd.Series(range(len(context)))%5).astype(str)\n",
    "\n",
    "# ------------- Customer feedback table ----------\n",
    "clusters = context[['city','city_key','province','region','olt_id','olt_key','cluster_id','cluster_key','cluster_name']].drop_duplicates().reset_index(drop=True)\n",
    "clusters['customer_id'] = ['CUST' + str(i).zfill(5) for i in range(len(clusters))]\n",
    "# Use province code for IDs to stay unique and readable\n",
    "clusters['_prov_code'] = clusters['city'].apply(province_code)\n",
    "clusters['home_pass'] = \"HP-\" + clusters['_prov_code'] + \"-\" + clusters.index.astype(str).str.zfill(5)\n",
    "clusters['home_connect_id'] = \"HC-\" + clusters['_prov_code'] + \"-\" + clusters.index.astype(str).str.zfill(6)\n",
    "clusters = clusters.drop(columns=['_prov_code'])\n",
    "\n",
    "clusters['join_date'] = pd.Timestamp(\"2020-01-01\") + pd.to_timedelta((clusters.index*30)%1000, unit='D')\n",
    "clusters['active_date'] = clusters['join_date']\n",
    "clusters['user_satisfaction_score'] = 0.8 + 0.2*np.random.rand(len(clusters))\n",
    "clusters['complaints_this_month'] = np.random.poisson(3, len(clusters))\n",
    "clusters['late_payment_count'] = np.random.randint(0,3, len(clusters))\n",
    "clusters['churn_flag'] = np.random.choice([0,1], len(clusters), p=[0.9,0.1])\n",
    "clusters['network_usage_gb'] = np.round(np.random.gamma(6, 20, size=len(clusters)), 1)  # GB\n",
    "clusters['customer_age'] = np.random.randint(18, 70, len(clusters))\n",
    "\n",
    "clusters['device_status'] = np.where(np.random.rand(len(clusters))<0.95, \"Online\", \"Offline\")\n",
    "clusters['device_link_os'] = np.random.choice([\"Up\",\"Down\",\"Flapping\"], size=len(clusters), p=[0.9,0.07,0.03])\n",
    "base = pd.Timestamp(\"2024-12-15\")\n",
    "clusters['last_online_ts'] = base + pd.to_timedelta(np.random.randint(0,400,size=len(clusters)), unit=\"h\")\n",
    "clusters['last_offline_ts'] = clusters['last_online_ts'] - pd.to_timedelta(np.random.randint(0,72,size=len(clusters)), unit=\"h\")\n",
    "clusters['offline_reason'] = np.random.choice([\"Power outage\",\"Fiber issue\",\"Maintenance\",\"Unknown\"], size=len(clusters), p=[0.20,0.30,0.25,0.25])\n",
    "\n",
    "feedback = clusters.rename(columns={'customer_id':'customer_id'})\n",
    "\n",
    "# --------------- Trend table (daily) ------------\n",
    "realtime['date'] = realtime['timestamp_1h'].dt.normalize()\n",
    "\n",
    "trend = realtime.groupby(['city','city_key','date'], as_index=False).agg(\n",
    "    link_loss_count=('link_loss_count','sum'),\n",
    "    bad_rsl_count=('bad_rsl_count','sum'),\n",
    "    high_temp_count=('high_temp_count','sum'),\n",
    "    dying_gasp_count=('dying_gasp_count','sum'),\n",
    "    total_onts=('ont_registered','sum')\n",
    ")\n",
    "\n",
    "trend['total_alarms'] = (\n",
    "    trend['link_loss_count'] + \n",
    "    trend['bad_rsl_count'] + \n",
    "    trend['high_temp_count'] + \n",
    "    trend['dying_gasp_count']\n",
    ")\n",
    "trend['fault_rate_daily'] = (trend['total_alarms'] / np.maximum(1, trend['total_onts'])).clip(lower=0)\n",
    "\n",
    "trend['link_loss_vs_yesterday'] = trend.groupby('city')['link_loss_count'].pct_change().fillna(0)\n",
    "trend['bad_rsl_vs_yesterday'] = trend.groupby('city')['bad_rsl_count'].pct_change().fillna(0)\n",
    "trend['high_temp_vs_yesterday'] = trend.groupby('city')['high_temp_count'].pct_change().fillna(0)\n",
    "trend['dying_gasp_vs_yesterday'] = trend.groupby('city')['dying_gasp_count'].pct_change().fillna(0)\n",
    "trend['region'] = trend['city'].map(region_map)\n",
    "\n",
    "# ---------------- Ticket table (daily) ----------\n",
    "ticket = trend[['city','city_key','date','region']].copy()\n",
    "ticket['ticket_open'] = (ticket.index % 10) + (ticket['city'].factorize()[0]) * 2\n",
    "ticket['ticket_closed'] = (ticket['ticket_open'] * 0.9).astype(int)\n",
    "ticket['active_ticket'] = (ticket['ticket_open'] - ticket['ticket_closed']).clip(lower=0)\n",
    "\n",
    "rng2 = np.random.default_rng(777)\n",
    "u = rng2.random(len(ticket))\n",
    "short = rng2.uniform(1, 8, size=len(ticket))\n",
    "mid   = rng2.uniform(8, 24, size=len(ticket))\n",
    "long  = rng2.uniform(24, 72, size=len(ticket))\n",
    "ticket['ticket_holding_time_hr'] = np.where(u < 0.7, short, np.where(u < 0.9, mid, long)).round(1)\n",
    "ticket['avg_ticket_duration_hr'] = (1.0 + (ticket['ticket_open'] % 4)).astype(float)\n",
    "ticket['total_downtime_duration_hr'] = (ticket['ticket_open'] * (ticket['avg_ticket_duration_hr'] * 0.8)).round(1)\n",
    "\n",
    "weights = np.array([0.4, 0.35, 0.15, 0.10]); weights = weights/weights.sum()\n",
    "def split_counts(n):\n",
    "    if n <= 0: return [0,0,0,0]\n",
    "    return list(np.random.multinomial(int(n), weights))\n",
    "split_vals = np.vstack([split_counts(n) for n in ticket['ticket_open'].values])\n",
    "ticket['issue_bad_connection'] = split_vals[:,0]\n",
    "ticket['issue_link_loss']     = split_vals[:,1]\n",
    "ticket['issue_high_temp']     = split_vals[:,2]\n",
    "ticket['issue_dying_gasp']    = split_vals[:,3]\n",
    "\n",
    "def top_rca(row):\n",
    "    counts = {\n",
    "        \"Bad Connection\": row['issue_bad_connection'],\n",
    "        \"Link Loss\": row['issue_link_loss'],\n",
    "        \"High Temp\": row['issue_high_temp'],\n",
    "        \"Dying Gasp\": row['issue_dying_gasp'],\n",
    "    }\n",
    "    return max(counts, key=counts.get)\n",
    "ticket['root_cause_top'] = ticket.apply(top_rca, axis=1)\n",
    "\n",
    "ticket['urgency_visit_count'] = (0.3 * ticket['active_ticket']).round().astype(int)\n",
    "\n",
    "ticket['case_id'] = [\"CASE-\" + str(100000 + i) for i in range(len(ticket))]\n",
    "ticket['close_subject'] = rng2.choice([\"Fiber repair\",\"Port reset\",\"Power restored\",\"Vendor escalation\",\"Config fix\"], size=len(ticket))\n",
    "ticket['vendor'] = rng2.choice([\"Huawei\",\"ZTE\",\"Nokia\"], size=len(ticket))\n",
    "ticket['action_taken'] = rng2.choice([\"Replace patchcore\",\"Clean connector\",\"Reboot OLT port\",\"Reroute traffic\",\"Dispatch field team\"], size=len(ticket))\n",
    "ticket['schedule_time'] = np.where(ticket['active_ticket']>0, (ticket['date'] + pd.Timedelta(days=1)).dt.strftime(\"%Y-%m-%d 09:00\"), ticket['date'].dt.strftime(\"%Y-%m-%d 13:00\"))\n",
    "ticket['duration_hours'] = ticket['avg_ticket_duration_hr']\n",
    "ticket['active_flag'] = (ticket['active_ticket']>0).astype(int)\n",
    "\n",
    "# Link to customers\n",
    "customer_pool = feedback['customer_id'].values\n",
    "sample_idx = rng2.integers(0, len(customer_pool), size=len(ticket))\n",
    "ticket['customer_id'] = customer_pool[sample_idx]\n",
    "\n",
    "ticket['rca_a'] = rng2.choice([\"Access\",\"Aggregation\",\"Backhaul\"], size=len(ticket))\n",
    "ticket['rca_g'] = rng2.choice([\"Power\",\"Physical\",\"Configuration\",\"Unknown\"], size=len(ticket))\n",
    "ticket['rca_d'] = rng2.choice([\"Patchcore cut\",\"SFP faulty\",\"Overheat\",\"Fiber attenuation\",\"N/A\"], size=len(ticket))\n",
    "ticket['indicator_status'] = rng2.choice([\"Open\",\"Monitoring\",\"Closed\"], size=len(ticket))\n",
    "\n",
    "weekday = ticket['date'].dt.dayofweek\n",
    "month = ticket['date'].dt.month\n",
    "season_boost = np.where(month.isin([1,2,3,11,12]), 1.1, 0.9)\n",
    "base_act = 10 + (weekday.isin([4,5]).astype(int))*5\n",
    "rng_act = np.random.default_rng(2025)\n",
    "ticket['new_activation'] = (base_act * season_boost + rng_act.integers(0,7,size=len(ticket))).astype(int)\n",
    "\n",
    "# ----------------- ML features table -----------------\n",
    "ml = realtime.copy()\n",
    "ml = ml.sort_values([\"city\",\"olt_id\",\"cluster_id\",\"timestamp_1h\"])\n",
    "# Predict next-hour outage\n",
    "ml['label_outage_1h'] = ml.groupby(['city','olt_id','cluster_id'])['outage_now'].shift(-1).fillna(0).astype(int)\n",
    "\n",
    "# Drop helpers not part of schema\n",
    "drop_cols = [c for c in ['outage_now','date','precursor_flag','stress'] if c in ml.columns]\n",
    "ml = ml.drop(columns=drop_cols)\n",
    "\n",
    "ml = ml[[\n",
    "    \"timestamp_1h\", \"city\", \"city_key\", \"province\", \"region\",\n",
    "    \"olt_id\", \"olt_key\", \"fdt_name\", \"fat_name\", \"cluster_id\", \"cluster_key\", \"cluster_name\",\n",
    "    \"ont_registered\",\"offline_ont_now\",\"offline_ont_ratio\",\n",
    "    \"link_loss_count\",\"bad_rsl_count\",\"high_temp_count\",\"dying_gasp_count\",\n",
    "    \"alarm_spike_flag\",\"trap_trend_score\",\"fault_rate\",\n",
    "    \"snr_avg\",\"rx_power_avg\",\"rx_power_avg_dbm\",\"temperature_avg_c\",\"temp_anomaly_score\",\n",
    "    \"hour_of_day\",\"day_of_week\",\"is_maintenance_window\",\n",
    "    \"label_outage_1h\"\n",
    "]]\n",
    "\n",
    "# ---------------- Dim tables (for BI) ---------------\n",
    "dim_city = pd.DataFrame({\n",
    "    'city_key': list(city_key_map.values()),\n",
    "    'city': list(city_key_map.keys()),              # province names\n",
    "    'province': [province_map[c] for c in city_key_map.keys()],\n",
    "    'region': [region_map[c] for c in city_key_map.keys()]\n",
    "}).sort_values('city_key')\n",
    "\n",
    "dim_olt = context[['olt_key','olt_id','city_key']].drop_duplicates().sort_values('olt_key')\n",
    "dim_cluster = context[['cluster_key','cluster_id','olt_key','city_key','cluster_name']].drop_duplicates().sort_values('cluster_key')\n",
    "\n",
    "# -------------------- Save outputs -------------------\n",
    "realtime_out = \"dataset/ews_network_realtime_status.parquet\"\n",
    "context_out  = \"dataset/ews_network_context_static.parquet\"\n",
    "trend_out    = \"dataset/ews_incident_trend_summary.parquet\"\n",
    "ticket_out_parquet = \"dataset/ews_ticket_summary.parquet\"\n",
    "feedback_out = \"dataset/ews_customer_feedback.parquet\"\n",
    "ml_out       = \"dataset/ews_ml_features_table.parquet\"\n",
    "city_dim_out = \"dataset/dim_city.csv\"\n",
    "olt_dim_out  = \"dataset/dim_olt.csv\"\n",
    "cluster_dim_out = \"dataset/dim_cluster.csv\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "import os\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "realtime.to_parquet(realtime_out, index=False)\n",
    "context.to_parquet(context_out, index=False)\n",
    "trend.to_parquet(trend_out, index=False)\n",
    "feedback.to_parquet(feedback_out, index=False)\n",
    "ml.to_parquet(ml_out, index=False)\n",
    "\n",
    "dim_city.to_csv(city_dim_out, index=False)\n",
    "dim_olt.to_csv(olt_dim_out, index=False)\n",
    "dim_cluster.to_csv(cluster_dim_out, index=False)\n",
    "\n",
    "# Ticket post-processing + save\n",
    "if \"schedule_visit\" not in ticket.columns:\n",
    "    ticket[\"schedule_visit\"] = np.where(\n",
    "        (ticket.get(\"urgency_visit_count\", 0) > 0) | (ticket.get(\"active_ticket\", 0) > 0),\n",
    "        \"Yes\", \"No\"\n",
    "    )\n",
    "\n",
    "if \"xl_id\" not in ticket.columns:\n",
    "    _codes = ticket['city'].apply(province_code)\n",
    "    ticket = ticket.sort_values([\"city\",\"date\"]).reset_index(drop=True)\n",
    "    seq = ticket.groupby([\"city\",\"date\"]).cumcount() + 1\n",
    "    ticket[\"xl_id\"] = \"XL-\" + _codes + \"-\" + ticket[\"date\"].dt.strftime(\"%Y%m%d\") + \"-\" + seq.astype(str).str.zfill(5)\n",
    "\n",
    "preferred = [\n",
    "    \"date\",\"city\",\"city_key\",\"region\",\"ticket_open\",\"ticket_closed\",\"active_ticket\",\n",
    "    \"ticket_holding_time_hr\",\"total_downtime_duration_hr\",\n",
    "    \"issue_bad_connection\",\"issue_link_loss\",\"issue_high_temp\",\"issue_dying_gasp\",\n",
    "    \"root_cause_top\",\"urgency_visit_count\",\"schedule_visit\",\n",
    "    \"case_id\",\"xl_id\",\"close_subject\",\"vendor\",\"action_taken\",\n",
    "    \"schedule_time\",\"duration_hours\",\"active_flag\",\"customer_id\",\n",
    "    \"rca_a\",\"rca_g\",\"rca_d\",\"indicator_status\",\n",
    "    \"new_activation\"\n",
    "]\n",
    "cols = [c for c in preferred if c in ticket.columns] + [c for c in ticket.columns if c not in preferred]\n",
    "ticket = ticket[cols]\n",
    "\n",
    "ticket.to_parquet(ticket_out_parquet, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12851172-7054-4552-8a70-a926af7b940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows (ML table): 3,578,976 â€” Positive rate: 0.4125% (target ~ 0.30%)\n",
      "Saved to 'dataset/' folder.\n"
     ]
    }
   ],
   "source": [
    "total_hours = len(ml)\n",
    "pos_rate = ml['label_outage_1h'].mean()\n",
    "print(f\"Rows (ML table): {total_hours:,} â€” Positive rate: {pos_rate:.4%} (target ~ {TARGET_OUTAGE_RATE:.2%})\")\n",
    "print(\"Saved to 'dataset/' folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
